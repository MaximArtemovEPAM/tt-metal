# DeepSeek Code Principles

This codebase separates model execution into three distinct stages, each of which can be run independently:
1. Convert PyTorch weights to TTNN tensor files
2. Generate ModelConfig dicts for prefill and decode modes
3. Load TTNN tensor files and add them to ModelConfig to create a RunConfig and execute the model

## Per-submodule model configs

Generated by static methods `prefill_model_config()` and `decode_model_config()` on each module class. Contains operator configurations as nested dicts:
```python
{
    "w1": {
        "memory_config": ttnn.DRAM_MEMORY_CONFIG,
        "program_config": [<list of program configs for different chunk sizes>],
        "compute_kernel_config": ttnn.experimental.tensor.CoreRangeSet(...)
    },
    "mul_activation": {
        "memory_config": ttnn.DRAM_MEMORY_CONFIG,
        "input_tensor_a_activations": [ttnn.UnaryOpType.SILU] # list because the ttnn.mul op expects a list
    },
    "all_reduce": {
        "cluster_axis": 0,
        "topology": ttnn.Topology.Ring,
        "dtype": ttnn.bfloat8_b
    }
}
```

Since prefill and decode modes typically have very different performance characteristics, we generate separate config dicts for each mode. This makes the configuration explicit and allows for easy mode-specific tuning.

### Static vs Dynamic Configuration

Model config functions should only contain static configurations that can be serialized to JSON. For dynamic configurations that depend on runtime parameters (e.g., sequence length):

- Add a comment to the model config function indicating something is dynamically set at runtime
- Implement the dynamic configuration in the module's constructor
- Override the config in the forward pass using keyword arguments

For example:
```python
# In prefill_model_config:
# program_config is dynamic - see __init__ for details
config["w1"] = {
    "memory_config": ttnn.DRAM_MEMORY_CONFIG,
    "compute_kernel_config": COMPUTE_KERNEL_CONFIG_HIFI2_FP16
}

# In __init__:
self.w1_pc = lambda seq_len: matmul_config(
    m=seq_len,
    k=dim // num_devices,
    n=n_w1_w3,
    grid_size=mlp1_3_grid,
    per_core_N=math.ceil(n_w1_w3 / (TILE_SIZE * dram_shard_grid_width))
)

# In forward:
output = ttnn.linear(x, program_config=self.w1_pc(current_seq_len), **cfg["w1"])
```

This approach keeps model configs serializable while allowing for dynamic runtime behavior.

ModelConfig supports **range patterns** for lists of submodules:
```python
{
    "model": {
        "layers": {
            "0-31": {
                "mlp": {
                    "w1": {
                        "memory_config": ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,
                        "program_config": ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig(...)
                    }
                }
            },
            "0-15": {
                "attention": {
                    "wq": {"memory_config": ttnn.DRAM_MEMORY_CONFIG}
                }
            },
            "16-31": {
                "attention": {
                    "wq": {"memory_config": ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG}
                }
            },
            "10": {
                "mlp": {
                    "experts": {"special_config": <custom TTNN object>}
                }
            }
        }
    }
}
```
These ranges are expanded when creating the RunConfig (e.g., to separate `model["layers"][0]["mlp"]["w1"]`, `model["layers"][1]["mlp"]["w1"]`, etc.). Both ranges (`0-31`) and single layers (`10`) are supported. Non-overlapping ranges allow different configurations for different layer groups. Overlapping ranges will raise an error.

## Runtime Config

Created at runtime by combining model configs with loaded TTNN weights using the `create_run_config()` function:
- Expands range patterns in model configs to match the actual model structure
- Loads TTNN tensors from disk based on a standard naming convention
- All config values are already TTNN objects (no string conversion needed)
- Adds loaded weight tensors directly to the config structure
- Returns a clean nested dict structure for easy access

## Module Requirements

Each module should implement:

### 1. `convert_weights(state_dict, output_path, mesh_device)` (static method)
- Converts PyTorch weights to TTNN format using a standard configuration:
  - dtype: `ttnn.bfloat8_b`
  - layout: `ttnn.TILE_LAYOUT`
  - memory_config: `ttnn.DRAM_MEMORY_CONFIG`
- Saves them to disk at the specified path with standard naming (e.g., `w1.weight`, `w2.weight`)
- Handles sharding across devices appropriately (typically column sharding for projections, row sharding for down projections)
- No need to return anything - weights are saved in a standard format

### 2. `prefill_model_config(hf_config, mesh_device) -> model_config` (static method)
- Generates static operator configurations for prefill mode
- Returns a nested dict with TTNN memory configs, static program configs, compute kernel configs, etc.
- Can use helper functions like `find_prefill_grid`, `dram_shard_core_grid_for_k`
- Should generate configs that match the standard weight format used by `convert_weights()`
- For dynamic configs (like sequence-dependent program configs), set to `None` and implement in constructor

### 3. `decode_model_config(hf_config, mesh_device) -> model_config` (static method)
- Generates static operator configurations for decode mode
- Typically uses L1 sharding and different matmul configurations than prefill
- Returns the same nested structure as prefill_model_config but with decode-optimized settings

### 4. Class constructor `__init__(self, mesh_device, hf_config)`
- Takes mesh_device and hf_config as parameters
- Implements dynamic configurations as instance methods/lambdas (e.g., for sequence-dependent program configs)
- Can store these for later use or perform any necessary initialization
- Good place for setting up non-weight tensors like kv_cache or dynamic program configs

### 5. `forward(self, x, cfg, mesh_device)`
- Executes the layer using the provided RunConfig
- Access mode via `cfg.get("mode")` if needed (though ideally not necessary)
- Uses clean dict expansion: `ttnn.linear(x, **cfg["w1"])`
- For dynamic configs, overrides with keyword arguments: `ttnn.linear(x, program_config=self.w1_pc(seq_len), **cfg["w1"])`
- Handles memory management (deallocations)

## Design Benefits

1. **Simplicity**: Just nested dicts throughout - no complex object conversion
2. **Clean Syntax**: Forward passes use `**cfg["op"]` instead of `**cfg.op.__dict__`
3. **Separation of Concerns**: Weight conversion and operator configuration are independent
4. **Reusability**: Same weights work with different operator configurations
5. **Hierarchical Support**: Range patterns allow clean configuration of deep models
6. **Direct Memory Usage**: No object conversion overhead - configs use TTNN objects directly
7. **Minimal Boilerplate**: Clean forward functions with `**cfg["op"]` expansion
8. **Serializability**: Static configs in model_config functions, dynamic configs in constructor
9. **Runtime Flexibility**: Dynamic program configs can adapt to sequence length and other runtime parameters

## Example Usage

```python
# Stage 1: Convert weights (saves to disk in standard format)
MLP_1D.convert_weights(torch_state_dict, Path("weights/mlp"), mesh_device)

# Stage 2: Generate operator configs (returns nested dicts with TTNN objects)
prefill_config = MLP_1D.prefill_model_config(hf_config, mesh_device)
decode_config = MLP_1D.decode_model_config(hf_config, mesh_device)

# Stage 3: Runtime execution
model_config = decode_config  # or prefill_config
cfg = create_run_config(model_config, "weights/mlp", mesh_device)
mlp = MLP_1D(mesh_device, hf_config)
output = mlp.forward(input_tensor, cfg, mesh_device)
```

For hierarchical models with ranges:
```python
# ModelConfig might contain:
config = {
    "model": {
        "layers": {
            "0-31": {
                "mlp": {"w1": {"memory_config": ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG}}
            },
            "0-15": {
                "attention": {"wq": {"memory_config": ttnn.DRAM_MEMORY_CONFIG}}
            },
            "16-31": {
                "attention": {"wq": {"memory_config": ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG}}
            },
            "10": {
                "mlp": {"special_layer": {"config": <custom TTNN object>}}  # Single layer override
            }
        }
    }
}

# create_run_config() will expand ranges and add weights:
# cfg["model"]["layers"][0]["mlp"]["w1"]["memory_config"] = ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG
# cfg["model"]["layers"][0]["mlp"]["w1"]["weight"] = <loaded TTNN tensor>
# cfg["model"]["layers"][1]["mlp"]["w1"]["memory_config"] = ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG
# cfg["model"]["layers"][1]["mlp"]["w1"]["weight"] = <loaded TTNN tensor>
# ... for all layers in each range
# cfg["model"]["layers"][10]["mlp"]["special_layer"]["config"] = <custom TTNN object>  # Just layer 10
```

## Implementation Notes

- Weight files follow a standard naming convention that matches the config keys (e.g., `w1.weight` file for `w1` config entries)
- The framework handles range expansion when creating runtime config, not individual modules
- Overlapping ranges in ModelConfig will raise an error during `create_run_config()`
- Runtime config is just a nested dict with loaded tensors added
- Modules should be specific rather than overly general. Different architectures or sharding strategies warrant different module implementations
- Validation happens when creating runtime config - ensuring weights exist and match expected shapes/sharding
- Dynamic configurations (like sequence-dependent program configs) should be implemented in the constructor and applied in the forward pass, not in the model config functions
