{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 06:01:31.716 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/maxim_artemov/.cache/ttnn,model_cache_path=/home/maxim_artemov/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-24 06:01:32.308] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.309] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.322] [info] [tt_cluster.cpp:177] [Device] Opening user mode device driver\n",
      "[2025-06-24 06:01:32.322] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.323] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.327] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.328] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.332] [info] [cluster.cpp:277] [SiliconDriver] Harvesting mask for chip 0 is 0x40 (physical layout: 0x40, logical: 0x40, simulated harvesting mask: 0x0).\n",
      "[2025-06-24 06:01:32.363] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
      "[2025-06-24 06:01:32.370] [info] [cluster.cpp:147] [SiliconDriver] Opening local chip ids/pci ids: {0}/[0] and remote chip ids {}\n",
      "[2025-06-24 06:01:32.375] [info] [cluster.cpp:1052] [SiliconDriver] Software version 6.0.0, Ethernet FW version 6.15.0 (Device 0)\n",
      "[2025-06-24 06:01:32.393] [info] [metal_context.cpp:72] [Metal] AI CLK for device 0 is:   1000 MHz\n",
      "[2025-06-24 06:01:32.685] [info] [device.cpp:1046] [Metal] Initializing device 0. Program cache is NOT enabled\n"
     ]
    }
   ],
   "source": [
    "import ttnn\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable program cache\n",
    "\n",
    "Enabling the program cache will speed up the execution of operations that run repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-24 06:01:33.605] [info] [mesh_device.cpp:518] [Metal] Enabling program cache on MeshDevice 1\n"
     ]
    }
   ],
   "source": [
    "device.enable_program_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1024\n",
    "k = 1024\n",
    "n = 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tensors a and b with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ttnn.rand((m, k), dtype=ttnn.bfloat16, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "b = ttnn.rand((k, n), dtype=ttnn.bfloat16, device=device, layout=ttnn.TILE_LAYOUT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b\n",
    "The operation will run longer the first time because the kernels need to get compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running the operation shows significant speed up by utilizing program caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the layout of matrix multiplication output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "print(output.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, matrix multiplication produces outputs in a tile layout. That is because it's much more efficient to use this layout for computing matrix multiplications on Tenstorrent accelerators compared to a row-major layout.\n",
    "\n",
    "And this is aslo why the logs show 2 tilize operations, as the inputs get automatically convered to the tile layout if they are in a row-major layout.\n",
    "\n",
    "Learn more about tile layout here: TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result of the matrix multiplication\n",
    "\n",
    "To inspect the results we will first convert to row-major layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing ttnn tensor\n",
      "shape: Shape([1024, 1024])\n",
      "chunk of a tensor:\n",
      "ttnn.Tensor([[258.00000, 256.00000,  ..., 250.00000, 256.00000]], shape=Shape([1, 32]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)\n"
     ]
    }
   ],
   "source": [
    "output = ttnn.to_layout(output, ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "print(\"Printing ttnn tensor\")\n",
    "print(f\"shape: {output.shape}\")\n",
    "print(f\"chunk of a tensor:\\n{output[:1, :32]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b by using more performant config\n",
    "By default, matrix multiplication might not be as effecient as it could be. To speed it up further, the user can specify how many cores they want matrix multiplication to use. This can speed up the operation significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ttnn.rand((m, k), dtype=ttnn.bfloat16, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "b = ttnn.rand((k, n), dtype=ttnn.bfloat16, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "a = ttnn.to_device(a, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "b = ttnn.to_device(b, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "\n",
    "a = ttnn.to_layout(a, ttnn.TILE_LAYOUT)\n",
    "b = ttnn.to_layout(b, ttnn.TILE_LAYOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to compile the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy a massive speed up on the subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-24 06:01:35.334] [info] [device.cpp:1093] [Metal] Closing device 0\n",
      "[2025-06-24 06:01:35.334] [info] [device.cpp:1440] [Metal] Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
