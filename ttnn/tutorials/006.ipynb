{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b9702787-10f3-41b2-bd93-7b40d3e53383",
            "metadata": {},
            "source": [
                "# Resnet Block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "dbd63fbb-f61c-48a4-85a8-1003586be3fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-06-26 05:32:23.634 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
                        "Config{cache_path=/home/maxim_artemov/.cache/ttnn,model_cache_path=/home/maxim_artemov/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import ttnn\n",
                "from ttnn.tracer import trace, visualize\n",
                "from ttnn.model_preprocessing import preprocess_model_parameters, fold_batch_norm2d_into_conv2d"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7bb35807-08e5-4de1-a86f-c960c4ffee81",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2025-06-26 05:32:23.698] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.714] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.719] [info] [tt_cluster.cpp:177] [Device] Opening user mode device driver\n",
                        "[2025-06-26 05:32:23.719] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.720] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.724] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.725] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.729] [info] [cluster.cpp:277] [SiliconDriver] Harvesting mask for chip 0 is 0x40 (physical layout: 0x40, logical: 0x40, simulated harvesting mask: 0x0).\n",
                        "[2025-06-26 05:32:23.765] [info] [pci_device.cpp:191] [SiliconDriver] Opened PCI device 0; KMD version: 2.0.0, IOMMU: disabled\n",
                        "[2025-06-26 05:32:23.775] [info] [cluster.cpp:147] [SiliconDriver] Opening local chip ids/pci ids: {0}/[0] and remote chip ids {}\n",
                        "[2025-06-26 05:32:23.779] [info] [cluster.cpp:1052] [SiliconDriver] Software version 6.0.0, Ethernet FW version 6.15.0 (Device 0)\n",
                        "[2025-06-26 05:32:23.797] [info] [metal_context.cpp:72] [Metal] AI CLK for device 0 is:   1000 MHz\n",
                        "[2025-06-26 05:32:24.074] [info] [device.cpp:1046] [Metal] Initializing device 0. Program cache is NOT enabled\n"
                    ]
                }
            ],
            "source": [
                "torch.manual_seed(0)\n",
                "device_params = {\"l1_small_size\": 24576}\n",
                "device = ttnn.CreateDevice(device_id=0, **device_params)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "162b41bd-cd48-44cc-90cb-0040393ff179",
            "metadata": {},
            "source": [
                "## Torch Module (from torchvision)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d48f30d-de73-41a9-83b3-30e7a0dadd89",
            "metadata": {},
            "outputs": [],
            "source": [
                "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> torch.nn.Conv2d:\n",
                "    \"\"\"3x3 convolution with padding\"\"\"\n",
                "    return torch.nn.Conv2d(\n",
                "        in_planes,\n",
                "        out_planes,\n",
                "        kernel_size=3,\n",
                "        stride=stride,\n",
                "        padding=dilation,\n",
                "        groups=groups,\n",
                "        bias=False,\n",
                "        dilation=dilation,\n",
                "    )\n",
                "\n",
                "\n",
                "class TorchBasicBlock(torch.nn.Module):\n",
                "    expansion: int = 1\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        inplanes: int,\n",
                "        planes: int,\n",
                "        stride: int = 1,\n",
                "        downsample=None,\n",
                "        groups: int = 1,\n",
                "        base_width: int = 64,\n",
                "        dilation: int = 1,\n",
                "        norm_layer=None,\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        if norm_layer is None:\n",
                "            norm_layer = torch.nn.BatchNorm2d\n",
                "        if groups != 1 or base_width != 64:\n",
                "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
                "        if dilation > 1:\n",
                "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
                "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
                "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
                "        self.bn1 = norm_layer(planes)\n",
                "        self.relu = torch.nn.ReLU(inplace=True)\n",
                "        self.conv2 = conv3x3(planes, planes)\n",
                "        self.bn2 = norm_layer(planes)\n",
                "        self.downsample = downsample\n",
                "        self.stride = stride\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        identity = x\n",
                "\n",
                "        out = self.conv1(x)\n",
                "        out = self.bn1(out)\n",
                "        out = self.relu(out)\n",
                "        out = self.conv2(out)\n",
                "        out = self.bn2(out)\n",
                "\n",
                "        if self.downsample is not None:\n",
                "            identity = self.downsample(x)\n",
                "\n",
                "        out += identity\n",
                "        out = self.relu(out)\n",
                "\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bcdcd7c2-fd79-4e60-ba51-6f2958ce8b04",
            "metadata": {},
            "source": [
                "## Create torch module and preprocess it to get ttnn parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "13d5d008-4e85-4e3e-8d80-62397f43b55d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-06-26 05:32:55.129 | DEBUG    | ttnn:manage_config:91 - Set ttnn.CONFIG.enable_logging to False\n",
                        "2025-06-26 05:32:55.130 | DEBUG    | ttnn:manage_config:91 - Set ttnn.CONFIG.enable_comparison_mode to False\n",
                        "2025-06-26 05:32:55.130 | WARNING  | ttnn.model_preprocessing:from_torch:533 - ttnn: model cache can be enabled by passing model_name argument to preprocess_model[_parameters] and setting env variable TTNN_CONFIG_OVERRIDES='{\"enable_model_cache\": true}'\n",
                        "2025-06-26 05:32:55.130 | WARNING  | ttnn.model_preprocessing:_initialize_model_and_preprocess_parameters:483 - Putting the model in eval mode\n",
                        "2025-06-26 05:32:55.131 | DEBUG    | ttnn:manage_config:94 - Restored ttnn.CONFIG.enable_comparison_mode to False\n",
                        "2025-06-26 05:32:55.131 | DEBUG    | ttnn:manage_config:94 - Restored ttnn.CONFIG.enable_logging to False\n"
                    ]
                }
            ],
            "source": [
                "torch_model = TorchBasicBlock(inplanes=64, planes=64, stride=1)\n",
                "torch_input_tensor = ttnn.to_torch(ttnn.rand((8, 64, 56, 56), dtype=ttnn.float32))\n",
                "state_dict = torch_model.state_dict()\n",
                "\n",
                "\n",
                "def create_custom_preprocessor(device):\n",
                "    def custom_preprocessor(torch_model, name, ttnn_module_args):\n",
                "        parameters = {}\n",
                "        conv_weight_1, conv_bias_1 = fold_batch_norm2d_into_conv2d(torch_model.conv1, torch_model.bn1)\n",
                "        parameters[\"conv1\"] = {}\n",
                "        parameters[\"conv2\"] = {}\n",
                "        parameters[\"conv1\"][\"weight\"] = ttnn.from_torch(conv_weight_1, dtype=ttnn.bfloat16)\n",
                "        parameters[\"conv1\"][\"bias\"] = ttnn.from_torch(torch.reshape(conv_bias_1, (1, 1, 1, -1)), dtype=ttnn.bfloat16)\n",
                "        conv_weight_2, conv_bias_2 = fold_batch_norm2d_into_conv2d(torch_model.conv2, torch_model.bn2)\n",
                "        parameters[\"conv2\"][\"weight\"] = ttnn.from_torch(conv_weight_2, dtype=ttnn.bfloat16)\n",
                "        parameters[\"conv2\"][\"bias\"] = ttnn.from_torch(torch.reshape(conv_bias_2, (1, 1, 1, -1)), dtype=ttnn.bfloat16)\n",
                "        return parameters\n",
                "\n",
                "    return custom_preprocessor\n",
                "\n",
                "\n",
                "parameters = preprocess_model_parameters(\n",
                "    initialize_model=lambda: torch_model, custom_preprocessor=create_custom_preprocessor(device), device=None\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92650dab-d639-4f3b-8239-63b663c5fe0b",
            "metadata": {},
            "source": [
                "## Display the parameters of the module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ad0923f-98c9-48b8-bd22-838558c8e211",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{\n",
                            "  conv1: {\n",
                            "    weight: ttnn.Tensor(shape=Shape([64, 64, 3, 3]), layout=Layout.ROW_MAJOR, dtype=DataType.BFLOAT16),\n",
                            "    bias: ttnn.Tensor(shape=Shape([1, 1, 1, 64]), layout=Layout.ROW_MAJOR, dtype=DataType.BFLOAT16)\n",
                            "  },\n",
                            "  conv2: {\n",
                            "    weight: ttnn.Tensor(shape=Shape([64, 64, 3, 3]), layout=Layout.ROW_MAJOR, dtype=DataType.BFLOAT16),\n",
                            "    bias: ttnn.Tensor(shape=Shape([1, 1, 1, 64]), layout=Layout.ROW_MAJOR, dtype=DataType.BFLOAT16)\n",
                            "  }\n",
                            "}"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "318ea15f-75d5-48c8-80ee-974c07cb6adc",
            "metadata": {},
            "source": [
                "## Display the traced torch graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "283e37c8-e1d5-41cf-8028-9f277093b081",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ExpatError",
                    "evalue": "not well-formed (invalid token): line 1, column 0",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVG\n\u001b[0;32m----> 2\u001b[0m \u001b[43mSVG\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/ttnn/model_resnet_block_graph.svg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/workspace/tt-metal/python_env/lib/python3.10/site-packages/IPython/core/display.py:320\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# because of @data.setter methods in\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# subclasses ensure url and filename are set\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# before assigning to self.data\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m metadata\n",
                        "File \u001b[0;32m~/workspace/tt-metal/python_env/lib/python3.10/site-packages/IPython/core/display.py:491\u001b[0m, in \u001b[0;36mSVG.data\u001b[0;34m(self, svg)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# parse into dom object\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minidom\n\u001b[0;32m--> 491\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mminidom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseString\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# get svg tag (should be 1)\u001b[39;00m\n\u001b[1;32m    493\u001b[0m found_svg \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mgetElementsByTagName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "File \u001b[0;32m/usr/lib/python3.10/xml/dom/minidom.py:2000\u001b[0m, in \u001b[0;36mparseString\u001b[0;34m(string, parser)\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expatbuilder\n\u001b[0;32m-> 2000\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexpatbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2002\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pulldom\n",
                        "File \u001b[0;32m/usr/lib/python3.10/xml/dom/expatbuilder.py:925\u001b[0m, in \u001b[0;36mparseString\u001b[0;34m(string, namespaces)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     builder \u001b[38;5;241m=\u001b[39m ExpatBuilder()\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/lib/python3.10/xml/dom/expatbuilder.py:223\u001b[0m, in \u001b[0;36mExpatBuilder.parseString\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    221\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParser()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_subset(string)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParseEscape:\n",
                        "\u001b[0;31mExpatError\u001b[0m: not well-formed (invalid token): line 1, column 0"
                    ]
                }
            ],
            "source": [
                "from IPython.display import SVG\n",
                "SVG('/tmp/ttnn/model_resnet_block_graph.svg')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eafa0bcd-caa8-4fb7-8d23-73605195256d",
            "metadata": {},
            "source": [
                "## Implement ttnn version of the module. Pass in the parameters into the constructor. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ab662b0-97e5-4645-90db-90d3f00c9f82",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv:\n",
                "    def __init__(\n",
                "        self,\n",
                "        conv_params,\n",
                "        input_shape,\n",
                "        parameters,\n",
                "        *,\n",
                "        act_block_h=None,\n",
                "        reshard=False,\n",
                "        deallocate=True,\n",
                "        height_sharding=True,\n",
                "        activation=\"\",\n",
                "        groups=1,\n",
                "        dtype=ttnn.bfloat16,\n",
                "    ) -> None:\n",
                "        self.weights = parameters[\"weight\"]\n",
                "        if \"bias\" in parameters:\n",
                "            self.bias = parameters[\"bias\"]\n",
                "        else:\n",
                "            self.bias = None\n",
                "        self.kernel_size = (self.weights.shape[2], self.weights.shape[3])\n",
                "        self.conv_params = conv_params\n",
                "        self.out_channels = self.weights.shape[0]\n",
                "        self.act_block_h = act_block_h\n",
                "        self.reshard = reshard\n",
                "        self.deallocate = deallocate\n",
                "        self.activation = activation\n",
                "        self.groups = groups\n",
                "        self.dtype = dtype\n",
                "        self.shard_layout = (\n",
                "            ttnn.TensorMemoryLayout.HEIGHT_SHARDED if height_sharding else ttnn.TensorMemoryLayout.BLOCK_SHARDED\n",
                "        )\n",
                "        self.input_shape = input_shape\n",
                "\n",
                "    def __call__(self, device, input_tensor):\n",
                "\n",
                "        conv_config = ttnn.Conv2dConfig(\n",
                "            dtype=self.dtype,\n",
                "            weights_dtype=ttnn.bfloat16,\n",
                "            math_fidelity=ttnn.MathFidelity.LoFi,\n",
                "            activation=self.activation,\n",
                "            shard_layout=self.shard_layout,\n",
                "            fp32_dest_acc_enabled=False,\n",
                "            packer_l1_accum_enabled=False,\n",
                "            deallocate_activation=self.deallocate,\n",
                "        )\n",
                "        if self.act_block_h is not None:\n",
                "            conv_config.act_block_h_override = self.act_block_h\n",
                "\n",
                "        [output_tensor, _out_height, _out_width, self.weights, self.bias] = ttnn.conv2d(\n",
                "            input_tensor=input_tensor,\n",
                "            weight_tensor=self.weights,\n",
                "            bias_tensor=self.bias,\n",
                "            in_channels=self.input_shape[3],\n",
                "            out_channels=self.out_channels,\n",
                "            device=device,\n",
                "            kernel_size=self.kernel_size,\n",
                "            stride=(self.conv_params[0], self.conv_params[1]),\n",
                "            padding=(self.conv_params[2], self.conv_params[3]),\n",
                "            batch_size=self.input_shape[0],\n",
                "            input_height=self.input_shape[1],\n",
                "            input_width=self.input_shape[2],\n",
                "            conv_config=conv_config,\n",
                "            groups=self.groups,\n",
                "            return_output_size=True,\n",
                "            return_prepared_device_weights=True\n",
                "        )\n",
                "\n",
                "        return output_tensor\n",
                "\n",
                "\n",
                "class TTNNBasicBlock:\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        parameters,\n",
                "    ) -> None:\n",
                "        self.conv1 = Conv([1, 1, 1, 1], [8, 56, 56, 64], parameters=parameters[\"conv1\"])\n",
                "        self.conv2 = Conv([1, 1, 1, 1], [8, 56, 56, 64], parameters=parameters[\"conv2\"])\n",
                "        if \"downsample\" in parameters:\n",
                "            self.downsample = parameters.downsample\n",
                "        else:\n",
                "            self.downsample = None\n",
                "\n",
                "    def __call__(self, x, device):\n",
                "        identity = x\n",
                "\n",
                "        out = self.conv1(device, x)\n",
                "        out = ttnn.relu(out)\n",
                "        out = self.conv2(device, out)\n",
                "\n",
                "        if self.downsample is not None:\n",
                "            identity = self.downsample(x)\n",
                "\n",
                "        identity = ttnn.reshape(identity, out.shape)\n",
                "\n",
                "        identity = ttnn.to_memory_config(\n",
                "            identity,\n",
                "            memory_config=ttnn.get_memory_config(out),\n",
                "            dtype=ttnn.bfloat16,\n",
                "        )\n",
                "        out = ttnn.add(out, identity, memory_config=ttnn.get_memory_config(out))\n",
                "        out = ttnn.relu(out)\n",
                "\n",
                "        return out\n",
                "\n",
                "def run_model(model, torch_input_tensor, device):\n",
                "    input_tensor = torch.permute(torch_input_tensor, (0, 2, 3, 1))\n",
                "    input_tensor = ttnn.from_torch(input_tensor, layout=ttnn.TILE_LAYOUT, dtype=ttnn.bfloat16, device=device)\n",
                "\n",
                "    output_tensor = model(input_tensor, device)\n",
                "    output_tensor = ttnn.from_device(output_tensor)\n",
                "    output_tensor = ttnn.to_torch(output_tensor)\n",
                "    output_tensor = torch.permute(output_tensor, (0, 3, 1, 2))\n",
                "    output_tensor = torch.reshape(output_tensor, torch_input_tensor.shape)\n",
                "    output_tensor = output_tensor.to(torch_input_tensor.dtype)\n",
                "    return output_tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb126b07-d9be-4a03-b1f7-4f71c4b6243b",
            "metadata": {},
            "source": [
                "## Run ttnn module and display the traced graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42a44ab6",
            "metadata": {},
            "outputs": [],
            "source": [
                "ttnn_model = TTNNBasicBlock(parameters)\n",
                "# with ttnn.tracer.trace(): #Issue 12638\n",
                "output_tensor = run_model(ttnn_model, torch_input_tensor, device=device)\n",
                "# ttnn.tracer.visualize(output_tensor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c252ad7-80a4-4d2f-bd47-91b2da1e5cc5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
                        "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n"
                    ]
                }
            ],
            "source": [
                "ttnn.close_device(device)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
